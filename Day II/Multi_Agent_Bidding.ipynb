{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c5bbdd"
      },
      "source": [
        "# üß† AI Agents Bootcamp: Multi-Agent Bidding with CrewAI\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vipbasil/aibootcamp/blob/main/Day%20II/Multi_Agent_Bidding.ipynb)\n",
        "\n",
        "This notebook is part of the **AI Agents Bootcamp** (23‚Äì27 June 2025).  \n",
        "It demonstrates how to:\n",
        "\n",
        "- üï∏Ô∏è Build a Multi-Agent System (MAS) using CrewAI\n",
        "- üóÇÔ∏è Define multiple agents with distinct roles and models\n",
        "- ‚öñÔ∏è Implement bidding logic for dynamic task allocation\n",
        "- üß† Use locally hosted LLMs (via Ollama + DeepSeek)\n",
        "- üìã Coordinate tasks and evaluate outcomes within a MAS pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19eeeee4"
      },
      "source": [
        "## ‚öôÔ∏è Step 1: Environment Setup\n",
        "This installs and runs the `ollama` backend and exposes the service via localtunnel tunnel. Make sure to:\n",
        "- Restart the runtime if needed\n",
        "- Use the ngrok alternative (if Cloudflare is blocked or throttled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssm5RJKID82Z"
      },
      "outputs": [],
      "source": [
        "%pip install ollama\n",
        "%pip install colab-xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RimdapHHa7b9"
      },
      "source": [
        "## üõ†Ô∏è System Info Tools (Optional)\n",
        "\n",
        "Installs utilities (`pciutils`, `lshw`) to inspect hardware specs ‚Äî useful for checking GPU/CPU availability in Colab or local runtime.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBW3P8YuECqz"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install pciutils lshw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsSHIndzbDOB"
      },
      "source": [
        "## üì¶ Ollama Installation\n",
        "\n",
        "Downloads and installs Ollama via the official shell script ‚Äî run this once per environment setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwFE191pEGaI"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5285c8c4"
      },
      "source": [
        "## üîß Step 2: Programmatic Model Management and Server Initialization\n",
        "\n",
        "In this section, we:\n",
        "- Import the required libraries for managing subprocesses, HTTP requests, and multithreading\n",
        "- Start the Ollama server programmatically using a background thread\n",
        "- Pull the required models (`deepseek-r1:7b`, `llama3`) using `ollama pull`\n",
        "- Optionally include fallback to a smaller model (`deepseek-r1:1.5b`)\n",
        "- Confirm the list of available models and test that the local Ollama server is running at `localhost:11434`\n",
        "\n",
        "üìå **Why it matters**: This sets up your local model infrastructure for agent interaction. You'll later reference `localhost:11434` in your agent definitions to connect to these models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqoaeOpaEZZ8"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "import threading\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRLABB5HZZo7"
      },
      "source": [
        "##  Launching the Ollama Server in Background\n",
        "\n",
        "Before using any model, we need to start the **Ollama inference server**, which listens by default on `localhost:11434`.\n",
        "\n",
        "This snippet:\n",
        "- Defines a Python function `run_ollama()` that launches `ollama serve`\n",
        "- Starts it in a **background thread**, so the notebook remains interactive\n",
        "- Allows the server to stay active without blocking further cells\n",
        "\n",
        "üõ†Ô∏è **Note**: You only need to run this once per session. If you restart your Colab, re-run this cell before using any models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f370227"
      },
      "outputs": [],
      "source": [
        "# Start the Ollama server\n",
        "def run_ollama():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "thread = threading.Thread(target=run_ollama)\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSnmd7ryZqNM"
      },
      "source": [
        "## üì• Pulling Models\n",
        "\n",
        "We download pre-trained models from the Ollama registry:\n",
        "- `deepseek-r1:7b` ‚Äì reasoning & code\n",
        "- `llama3` ‚Äì general-purpose assistant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6723baed"
      },
      "outputs": [],
      "source": [
        "# Download the deepseek-r1:7b distilled model\n",
        "!ollama pull deepseek-r1:7b\n",
        "!ollama pull llama3\n",
        "!ollama pull deepseek-coder:6.7b\n",
        "# If this doesn't work, you can uncomment the below code to download a smaller model- deepseek-r1:1.5b\n",
        "# !ollama pull deepseek-r1:1.5b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eZ8XjjMZ3ip"
      },
      "source": [
        "## ü™∂ Pulling Lightweight SLMs\n",
        "\n",
        "These small models are ideal for fast local agents and low-resource environments:\n",
        "- `phi3:mini`, `tinyllama` ‚Äì ultra-small general models\n",
        "- `gemma:2b` ‚Äì Google's compact chat model\n",
        "- `deepseek-r1:1.5b` ‚Äì distilled reasoning model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbNtHtlRHlWG"
      },
      "outputs": [],
      "source": [
        "!ollama pull phi3:mini\n",
        "!ollama pull tinyllama\n",
        "!ollama pull gemma:2b\n",
        "!ollama pull deepseek-r1:1.5b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLHS7WUwaMNk"
      },
      "source": [
        "## üîå Test Ollama Server\n",
        "\n",
        "Sends a test request to verify the Ollama server is running on `localhost:11434`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aae1987"
      },
      "outputs": [],
      "source": [
        "!curl http://127.0.0.1:11434"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5JaeB4YaAlx"
      },
      "source": [
        "## üìÑ Check Installed Models\n",
        "\n",
        "Lists all models currently downloaded and available in your local Ollama environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9809d934"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504891b1"
      },
      "source": [
        "# üß† Starting the CrewAI Section\n",
        "\n",
        "##Now we define agents using CrewAI, connected to our locally running Ollama models.  \n",
        "##This enables multi-agent workflows powered by lightweight, self-hosted LLMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f26b8d0b"
      },
      "outputs": [],
      "source": [
        "# @title üë®‚Äçü¶Ø Run this cell to hide all warnings (optional)\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# To avoid the restart session warning in Colab, exclude the PIL and\n",
        "# pydevd_plugins packages from being imported. This is fine because\n",
        "# we didn't execute the code in the kernel session afterward.\n",
        "\n",
        "# import sys\n",
        "# sys.modules.pop('PIL', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Install Project Dependencies\n",
        "\n",
        "This cell installs the required Python packages to run the multi-agent system with CrewAI and LangChain integrations.  \n",
        "It includes:\n",
        "\n",
        "- `crewAI`: the main framework for defining agents and task flows  \n",
        "- `crewai_tools`: additional utilities to extend agent capabilities  \n",
        "- `langchain_*`: integrations for Groq, Anthropic, and other backends  \n",
        "- `cohere`: optional LLM support via Cohere API  \n",
        "- `--quiet`: suppresses output for a cleaner notebook  \n",
        "- `pip show`: verifies that all packages are successfully installed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ca86609"
      },
      "outputs": [],
      "source": [
        "# @title ‚¨áÔ∏è Install project dependencies by running this cell\n",
        "%pip install git+https://github.com/joaomdmoura/crewAI.git --quiet\n",
        "%pip install crewai_tools langchain_groq langchain_anthropic langchain_community cohere --quiet\n",
        "print(\"---\")\n",
        "%pip show crewAI crewai_tools langchain_groq langchain_anthropic langchain_community cohere"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "577d53c4"
      },
      "source": [
        "## üß© Step 3: CrewAI Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88866d05"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from textwrap import dedent\n",
        "from crewai import LLM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb30a521"
      },
      "source": [
        "## Define Agents\n",
        "In CrewAI, agents are autonomous entities designed to perform specific roles and achieve particular goals. Each agent uses a language model (LLM) and may have specialized tools to help execute tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configure the LLM Interface\n",
        "\n",
        "This line initializes the LLM wrapper used by CrewAI to route all language model calls to a local Ollama instance.  \n",
        "\n",
        "Key components:\n",
        "- `model=\"ollama/deepseek-r1:7b\"`: Specifies the local model name to use (e.g. `deepseek-r1:7b`)\n",
        "- `base_url=\"http://127.0.0.1:11434\"`: Connects to the locally running Ollama server (default port)\n",
        "\n",
        "```python\n",
        "llm = LLM(model=\"ollama/deepseek-r1:7b\", base_url=\"http://127.0.0.1:11434\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM2l-5VqZqYW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "llm = LLM(model=\"ollama/deepseek-r1:7b\", base_url=\"http://127.0.0.1:11434\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Define Agent Specifications\n",
        "\n",
        "This dictionary defines the configuration for each agent in your multi-agent system (MAS). Each agent includes:\n",
        "\n",
        "- `role`: the functional label of the agent\n",
        "- `goal`: the objective it should pursue\n",
        "- `backstory`: a narrative that helps guide agent reasoning (contextual prompt)\n",
        "- `llm`: the local model the agent will use (via Ollama)\n",
        "\n",
        "Three agents are specified:\n",
        "\n",
        "1. **Planner** ‚Äî responsible for system-level planning  \n",
        "2. **Coder** ‚Äî writes and tests backend code  \n",
        "3. **Reviewer** ‚Äî checks outputs for errors and logic bugs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQgm6uR6vMI_"
      },
      "outputs": [],
      "source": [
        "agent_specs = {\n",
        "    \"Planner\": {\n",
        "        \"role\": \"Planner\",\n",
        "        \"goal\": \"Create structured development plans.\",\n",
        "        \"backstory\": \"Expert in designing system workflows.\",\n",
        "        \"llm\" : \"ollama/deepseek-r1:7b\"\n",
        "    },\n",
        "    \"Coder\": {\n",
        "        \"role\": \"Coder\",\n",
        "        \"goal\": \"Write and test code effectively.\",\n",
        "        \"backstory\": \"Backend developer with API focus.\",\n",
        "        \"llm\" : \"ollama/deepseek-coder:6.7b\"\n",
        "    },\n",
        "    \"Reviewer\": {\n",
        "        \"role\": \"Reviewer\",\n",
        "        \"goal\": \"Review outputs and catch issues.\",\n",
        "        \"backstory\": \"Critical code reviewer and tester.\",\n",
        "        \"llm\" : \"ollama/deepseek-r1:7b\"\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Instantiate Agents from Specification\n",
        "\n",
        "This block dynamically creates agent instances from the `agent_specs` dictionary using a `for` loop.\n",
        "\n",
        "- Each agent is initialized with its role, goal, backstory, and LLM model.\n",
        "- All agents are added to the `_agents` list, which is later passed to the `Crew` object.\n",
        "- Their names are stored in `_agent_names` for easy reference or mapping.\n",
        "- The LLM is configured per agent using the local Ollama server and the agent-specific model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujzVu0rzZifz"
      },
      "outputs": [],
      "source": [
        "\n",
        "_agents = []\n",
        "_agent_names = []\n",
        "for name, spec in agent_specs.items():\n",
        "    _agent_names.append(name)\n",
        "    _agents.append( Agent(\n",
        "        role=spec[\"role\"],\n",
        "        goal=spec[\"goal\"],\n",
        "        backstory=spec[\"backstory\"],\n",
        "        verbose=True,\n",
        "        llm=LLM(model=spec[\"llm\"], base_url=\"http://127.0.0.1:11434\")\n",
        "    ))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6272d6"
      },
      "source": [
        "## ‚öôÔ∏è Define Task List\n",
        "\n",
        "This block prepares the task data to be assigned to agents.  \n",
        "Each task includes:\n",
        "\n",
        "- `description`: a short description of the task objective  \n",
        "- `type`: used to match the task with a suitable agent role (e.g., `plan`, `code`, `review`)  \n",
        "- `complexity`: an integer from 1‚Äì5 indicating task difficulty (used for bidding or scoring)\n",
        "\n",
        "You can later convert these tasks into `Task` objects and assign them dynamically.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQdlMcQBZ7L3"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ 2. Define tasks\n",
        "from crewai import Task\n",
        "from textwrap import dedent\n",
        "\n",
        "task_data = [\n",
        "    {\"description\": \"Design the user login flow\", \"type\": \"plan\", \"complexity\": 2},\n",
        "    {\"description\": \"Write the login API using FastAPI\", \"type\": \"code\", \"complexity\": 3},\n",
        "    {\"description\": \"Review the login module for bugs\", \"type\": \"review\", \"complexity\": 1},\n",
        "    {\"description\": \"Draft a database schema for user roles\", \"type\": \"plan\", \"complexity\": 3},\n",
        "    {\"description\": \"Implement user roles in backend\", \"type\": \"code\", \"complexity\": 4}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Define LLM Response Function\n",
        "\n",
        "This function sends a prompt to the local Ollama API endpoint and retrieves a text completion using a specified model (`deepseek-r1:7b`). It acts as a bridge between your logic and the LLM.\n",
        "\n",
        "### Behavior:\n",
        "- Sends a `POST` request to `http://127.0.0.1:11434/v1/completions`\n",
        "- Uses a simple prompt/response format\n",
        "- Parses the result and returns the model‚Äôs response as plain text\n",
        "- Includes basic error handling and a fallback agent name (`Planner`) if something fails\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cw-Bd7-poW_"
      },
      "outputs": [],
      "source": [
        "def get_response(prompt):\n",
        "    url = \"http://127.0.0.1:11434/v1/completions\"\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    data = {\n",
        "        \"prompt\": prompt,\n",
        "        \"model\": \"deepseek-r1:7b\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "        if response.status_code == 200:\n",
        "            response_data = response.json()\n",
        "            return response_data.get(\"choices\", [{}])[0].get(\"text\", \"\").strip()\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Error: HTTP {response.status_code}\")\n",
        "            print(response.text)\n",
        "            return \"Planner\"  # fallback to a valid agent name\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Request failed: {e}\")\n",
        "        return \"PlannerBot\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Choose Best Agent via LLM Prompt\n",
        "\n",
        "This function constructs a natural language prompt describing a task and a list of agents, then uses a language model (`llm_func`) to decide which agent is the best fit.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Agent Matching**: relies on each agent‚Äôs defined `goal` from `agent_specs`\n",
        "- **Prompt Engineering**: creates a prompt that includes the task description, type, and complexity\n",
        "- **LLM Decision**: expects the LLM to return only the exact name of the chosen agent\n",
        "- **Post-processing**: strips and parses the raw LLM response for use in assignment logic\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dEPDH2HpwrT"
      },
      "outputs": [],
      "source": [
        "def choose_best_agent(task, agents, llm_func):\n",
        "    agent_descriptions = \"\\n\".join([\n",
        "    f\"{i+1}. {name}: {spec['goal']}\" for i, (name, spec) in enumerate(agent_specs.items())\n",
        "])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "You are a task allocator AI.\n",
        "\n",
        "Your job is to assign the following task to the most appropriate agent based on their expertise.\n",
        "\n",
        "üîß Task:\n",
        "\\\"{task['description']}\\\" (Type: {task['type']}, Complexity: {task['complexity']})\n",
        "\n",
        "üß† Agents Available:\n",
        "{agent_descriptions}\n",
        "\n",
        "üëâ Question:\n",
        "Which agent is the best fit for this task? Respond only with the agent's **name**, exactly as written.\n",
        "\"\"\"\n",
        "    print(prompt)\n",
        "    response = llm_func(prompt).split(\"</think>\")[1]\n",
        "\n",
        "    return response.strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Generate Task Objects with LLM-Based Assignment\n",
        "\n",
        "This loop transforms each raw task from `task_data` into a formal `Task` object and assigns it to the most suitable agent. The decision is made by querying the LLM via `choose_best_agent`.\n",
        "\n",
        "### What happens:\n",
        "- Each task is passed to the `choose_best_agent()` function\n",
        "- The returned agent name is used to look up the corresponding agent instance\n",
        "- A new `Task` object is created using `crewai.Task`\n",
        "- The `description` and `expected_output` are passed through `dedent()` to clean up indentation\n",
        "- Each task is printed with a confirmation message and stored in `task_objects`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0abHW4cv6Y7"
      },
      "outputs": [],
      "source": [
        "from crewai import Task\n",
        "from textwrap import dedent\n",
        "\n",
        "task_objects = []\n",
        "\n",
        "for task in task_data:\n",
        "    selected_name = choose_best_agent(task, agent_specs, get_response)\n",
        "    print(selected_name)\n",
        "    selected_agent = _agents[_agent_names.index(selected_name)]#.get(selected_name, list(agents.values())[0])  # fallback\n",
        "\n",
        "    task_obj = Task(\n",
        "        description=dedent(task[\"description\"]),\n",
        "        expected_output=dedent(f\"Provide a full solution for: {task['description']}\"),\n",
        "        agent=selected_agent\n",
        "    )\n",
        "    task_objects.append(task_obj)\n",
        "    print(f\"‚úÖ Assigned '{task['description']}' to {selected_name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Create and Launch the Agent Crew\n",
        "\n",
        "This block initializes the full multi-agent execution environment using the `CrewAI` framework. It ties together the agents and tasks, defines the coordination strategy, and executes the workflow.\n",
        "\n",
        "### Components:\n",
        "- `LLM(...)`: re-initializes the LLM interface for compatibility (can be reused or shared)\n",
        "- `Crew(...)`: constructs the execution unit with:\n",
        "  - `agents`: a list of previously instantiated agents\n",
        "  - `tasks`: a list of assigned tasks\n",
        "  - `process`: execution strategy (`sequential` or `parallel`)\n",
        "- `crew.kickoff()`: begins the workflow, assigning each task to its agent\n",
        "- `result`: stores the outcome of all tasks, which is printed after execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnXYdiBDq2YR"
      },
      "outputs": [],
      "source": [
        "from crewai import Crew, Process\n",
        "from crewai import LLM\n",
        "\n",
        "llm = LLM(model=\"ollama/deepseek-r1:7b\", base_url=\"http://127.0.0.1:11434\")\n",
        "crew = Crew(\n",
        "    agents=_agents,\n",
        "    tasks=task_objects,\n",
        "    process=Process.sequential,\n",
        "\n",
        "\n",
        ")\n",
        "#print(list(agents.values()))\n",
        "result = crew.kickoff()\n",
        "print(\"üß† Final Result:\\n\", result)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñ•Ô∏è Display Crew Results as Formatted Markdown\n",
        "\n",
        "After the agents have completed their tasks, this block renders the results in a clean, readable Markdown format directly inside the notebook.\n",
        "\n",
        "### Purpose:\n",
        "- Uses `IPython.display.Markdown` to pretty-print structured output\n",
        "- Assumes `result.raw` contains the full textual response from the `Crew` execution\n",
        "- You can replace `result.raw` with `str(result)` or another attribute depending on how `CrewAI` returns output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "889206a0"
      },
      "outputs": [],
      "source": [
        "# @title üñ•Ô∏è Display the results of your crew as markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "markdown_text = result.raw  # Adjust this based on the actual attribute\n",
        "\n",
        "# Display the markdown content\n",
        "display(Markdown(markdown_text))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
