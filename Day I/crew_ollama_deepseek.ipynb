{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5c5bbdd"
      },
      "source": [
        "# üß† AI Agents Bootcamp: Running DeepSeek with CrewAI and Ollama\n",
        "This notebook is part of the AI Agents Bootcamp (23‚Äì27 June 2025) ‚Äî it shows how to:\n",
        "- Set up the `Ollama` environment in Colab\n",
        "- Run `DeepSeek` models locally for use in CrewAI/MAS pipelines\n",
        "- Prepare agents that use locally-hosted LLMs with memory and tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19eeeee4"
      },
      "source": [
        "## ‚öôÔ∏è Step 1: Environment Setup\n",
        "This installs and runs the `ollama` backend and exposes the service via localtunnel tunnel. Make sure to:\n",
        "- Restart the runtime if needed\n",
        "- Use the ngrok alternative (if Cloudflare is blocked or throttled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssm5RJKID82Z"
      },
      "outputs": [],
      "source": [
        "%pip install ollama\n",
        "%pip install colab-xterm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üõ†Ô∏è System Info Tools (Optional)\n",
        "\n",
        "Installs utilities (`pciutils`, `lshw`) to inspect hardware specs ‚Äî useful for checking GPU/CPU availability in Colab or local runtime.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RimdapHHa7b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install pciutils lshw"
      ],
      "metadata": {
        "id": "eBW3P8YuECqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Ollama Installation\n",
        "\n",
        "Downloads and installs Ollama via the official shell script ‚Äî run this once per environment setup."
      ],
      "metadata": {
        "id": "CsSHIndzbDOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "AwFE191pEGaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5285c8c4"
      },
      "source": [
        "## üîß Step 2: Programmatic Model Management and Server Initialization\n",
        "\n",
        "In this section, we:\n",
        "- Import the required libraries for managing subprocesses, HTTP requests, and multithreading\n",
        "- Start the Ollama server programmatically using a background thread\n",
        "- Pull the required models (`deepseek-r1:7b`, `llama3`) using `ollama pull`\n",
        "- Optionally include fallback to a smaller model (`deepseek-r1:1.5b`)\n",
        "- Confirm the list of available models and test that the local Ollama server is running at `localhost:11434`\n",
        "\n",
        "üìå **Why it matters**: This sets up your local model infrastructure for agent interaction. You'll later reference `localhost:11434` in your agent definitions to connect to these models.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import subprocess\n",
        "import requests\n",
        "import json\n",
        "import threading\n",
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "cqoaeOpaEZZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Launching the Ollama Server in Background\n",
        "\n",
        "Before using any model, we need to start the **Ollama inference server**, which listens by default on `localhost:11434`.\n",
        "\n",
        "This snippet:\n",
        "- Defines a Python function `run_ollama()` that launches `ollama serve`\n",
        "- Starts it in a **background thread**, so the notebook remains interactive\n",
        "- Allows the server to stay active without blocking further cells\n",
        "\n",
        "üõ†Ô∏è **Note**: You only need to run this once per session. If you restart your Colab, re-run this cell before using any models.\n"
      ],
      "metadata": {
        "id": "QRLABB5HZZo7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f370227"
      },
      "outputs": [],
      "source": [
        "# Start the Ollama server\n",
        "def run_ollama():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "thread = threading.Thread(target=run_ollama)\n",
        "thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì• Pulling Models\n",
        "\n",
        "We download pre-trained models from the Ollama registry:\n",
        "- `deepseek-r1:7b` ‚Äì reasoning & code\n",
        "- `llama3` ‚Äì general-purpose assistant"
      ],
      "metadata": {
        "id": "QSnmd7ryZqNM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6723baed"
      },
      "outputs": [],
      "source": [
        "# Download the deepseek-r1:7b distilled model\n",
        "!ollama pull deepseek-r1:7b\n",
        "!ollama pull llama3\n",
        "# If this doesn't work, you can uncomment the below code to download a smaller model- deepseek-r1:1.5b\n",
        "# !ollama pull deepseek-r1:1.5b"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü™∂ Pulling Lightweight SLMs\n",
        "\n",
        "These small models are ideal for fast local agents and low-resource environments:\n",
        "- `phi3:mini`, `tinyllama` ‚Äì ultra-small general models\n",
        "- `gemma:2b` ‚Äì Google's compact chat model\n",
        "- `deepseek-r1:1.5b` ‚Äì distilled reasoning model"
      ],
      "metadata": {
        "id": "0eZ8XjjMZ3ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull phi3:mini\n",
        "!ollama pull tinyllama\n",
        "!ollama pull gemma:2b\n",
        "!ollama pull deepseek-r1:1.5b"
      ],
      "metadata": {
        "id": "rbNtHtlRHlWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîå Test Ollama Server\n",
        "\n",
        "Sends a test request to verify the Ollama server is running on `localhost:11434`."
      ],
      "metadata": {
        "id": "SLHS7WUwaMNk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aae1987"
      },
      "outputs": [],
      "source": [
        "!curl http://127.0.0.1:11434"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìÑ Check Installed Models\n",
        "\n",
        "Lists all models currently downloaded and available in your local Ollama environment."
      ],
      "metadata": {
        "id": "V5JaeB4YaAlx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9809d934"
      },
      "outputs": [],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504891b1"
      },
      "source": [
        "# üß† Starting the CrewAI Section\n",
        "\n",
        "##Now we define agents using CrewAI, connected to our locally running Ollama models.  \n",
        "##This enables multi-agent workflows powered by lightweight, self-hosted LLMs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f26b8d0b"
      },
      "outputs": [],
      "source": [
        "# @title üë®‚Äçü¶Ø Run this cell to hide all warnings (optional)\n",
        "# Warning control\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# To avoid the restart session warning in Colab, exclude the PIL and\n",
        "# pydevd_plugins packages from being imported. This is fine because\n",
        "# we didn't execute the code in the kernel session afterward.\n",
        "\n",
        "# import sys\n",
        "# sys.modules.pop('PIL', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ca86609"
      },
      "outputs": [],
      "source": [
        "# @title ‚¨áÔ∏è Install project dependencies by running this cell\n",
        "%pip install git+https://github.com/joaomdmoura/crewAI.git --quiet\n",
        "%pip install crewai_tools langchain_openai langchain_groq langchain_anthropic langchain_community cohere --quiet\n",
        "print(\"---\")\n",
        "%pip show crewAI crewai_tools langchain_openai langchain_groq langchain_anthropic langchain_community cohere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f36bef0"
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "577d53c4"
      },
      "source": [
        "## üß© Step 3: CrewAI Integration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88866d05"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "\n",
        "import os\n",
        "from getpass import getpass\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from textwrap import dedent\n",
        "from langchain_ollama import ChatOllama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb30a521"
      },
      "source": [
        "## Define Agents\n",
        "In CrewAI, agents are autonomous entities designed to perform specific roles and achieve particular goals. Each agent uses a language model (LLM) and may have specialized tools to help execute tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c416f4fe"
      },
      "outputs": [],
      "source": [
        "# @title üïµüèª Define your agents\n",
        "\n",
        "from crewai import Agent\n",
        "from textwrap import dedent\n",
        "from langchain_ollama import ChatOllama\n",
        "# Define LLM (OpenAI used here; replace as needed)\n",
        "from crewai import LLM\n",
        "\n",
        "llm = LLM(model=\"ollama/tinyllama:latest\", base_url=\"http://127.0.0.1:11434\")\n",
        "\n",
        "# Agent 1: Researcher\n",
        "agent_1 = Agent(\n",
        "    role=dedent(\"\"\"Researcher\"\"\"),\n",
        "    goal=dedent(\"\"\"Identify reliable, up-to-date information on a technical topic.\"\"\"),\n",
        "    backstory=dedent(\"\"\"You are an experienced research assistant skilled at finding credible sources, synthesizing data, and distilling it into key insights.\"\"\"),\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    max_iter=3,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Agent 2: Analyst\n",
        "agent_2 = Agent(\n",
        "    role=dedent(\"\"\"Analyst\"\"\"),\n",
        "    goal=dedent(\"\"\"Interpret research findings and draw meaningful conclusions.\"\"\"),\n",
        "    backstory=dedent(\"\"\"You are a data-driven thinker who turns raw information into structured, actionable insights. You are logical and detail-oriented.\"\"\"),\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    max_iter=3,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Agent 3: Writer\n",
        "agent_3 = Agent(\n",
        "    role=dedent(\"\"\"Technical Writer\"\"\"),\n",
        "    goal=dedent(\"\"\"Craft a well-structured, engaging article based on the insights provided by the team.\"\"\"),\n",
        "    backstory=dedent(\"\"\"You are a clear and persuasive communicator who turns complex ideas into accessible writing for both technical and general audiences.\"\"\"),\n",
        "    allow_delegation=False,\n",
        "    verbose=True,\n",
        "    max_iter=3,\n",
        "    llm=llm\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef6272d6"
      },
      "source": [
        "## Define Tasks\n",
        "Tasks in CrewAI are specific assignments given to agents, detailing the actions they need to perform to achieve a particular goal. Tasks can have dependencies and context, and can be executed asynchronously to ensure an efficient workflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8249b567"
      },
      "outputs": [],
      "source": [
        "# @title üìù Define your tasks\n",
        "from crewai import Task\n",
        "from textwrap import dedent\n",
        "\n",
        "# Task 1: Researcher\n",
        "task_1 = Task(\n",
        "    description=dedent(\"\"\"\n",
        "        Research recent developments in open-source large language models (LLMs), with a focus on DeepSeek, its capabilities, and recent benchmarks. Identify 2‚Äì3 major advantages compared to closed-source models.\n",
        "    \"\"\"),\n",
        "    expected_output=dedent(\"\"\"\n",
        "        A structured summary (~200 words) covering:\n",
        "        - What DeepSeek is and who developed it\n",
        "        - Key features or innovations\n",
        "        - At least two comparative strengths of open-source LLMs\n",
        "    \"\"\"),\n",
        "    agent=agent_1,\n",
        ")\n",
        "\n",
        "# Task 2: Analyst\n",
        "task_2 = Task(\n",
        "    description=dedent(\"\"\"\n",
        "        Analyze the research findings about DeepSeek and extract educational implications. Focus on how this technology could benefit educators, students, or institutions using local infrastructure.\n",
        "    \"\"\"),\n",
        "    expected_output=dedent(\"\"\"\n",
        "        A short analytical breakdown (~150 words) including:\n",
        "        - 2‚Äì3 practical use cases in education\n",
        "        - The impact of offline/local LLMs on cost and data privacy\n",
        "        - Any challenges or limitations to note\n",
        "    \"\"\"),\n",
        "    agent=agent_2,\n",
        "    context=[task_1],\n",
        ")\n",
        "\n",
        "# Task 3: Writer\n",
        "task_3 = Task(\n",
        "    description=dedent(\"\"\"\n",
        "        Write a clear, engaging blog post summarizing the findings and analysis into a cohesive article titled:\n",
        "        \"How Open-Source AI Like DeepSeek Is Changing Education.\"\n",
        "    \"\"\"),\n",
        "    expected_output=dedent(\"\"\"\n",
        "        A 400‚Äì500 word markdown-formatted blog post including:\n",
        "        - Title and subtitle\n",
        "        - Introduction\n",
        "        - Three core paragraphs with bullet points if needed\n",
        "        - Conclusion and future outlook\n",
        "    \"\"\"),\n",
        "    agent=agent_3,\n",
        "    context=[task_2],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bb55322"
      },
      "outputs": [],
      "source": [
        "print(\"## üë• Welcome to the DeepSeek Research Crew\")\n",
        "print('-------------------------------------------')\n",
        "\n",
        "# Input variables for tasks\n",
        "var_1 = input(\"üîç Topic or model to explore (e.g., DeepSeek)?\\n\")\n",
        "var_2 = input(\"üéØ Who is the target audience (e.g., educators, developers)?\\n\")\n",
        "var_3 = input(\"üß† What‚Äôs the intended output format (e.g., article, summary)?\\n\")\n",
        "\n",
        "print('-------------------------------------------')\n",
        "print(f\"‚úîÔ∏è Input received:\\nModel: {var_1}\\nAudience: {var_2}\\nOutput Type: {var_3}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c658581"
      },
      "outputs": [],
      "source": [
        "# @title üöÄ Get your crew to work!\n",
        "def main():\n",
        "    # Instantiate your crew with a sequential process\n",
        "    crew = Crew(\n",
        "        agents=[agent_1, agent_2, agent_3],\n",
        "        tasks=[task_1, task_2, task_3],\n",
        "        verbose=True,  # You can set it to True or False\n",
        "        # ‚Üë indicates the verbosity level for logging during execution.\n",
        "        process=Process.sequential,\n",
        "        planning_llm=llm\n",
        "        # ‚Üë the process flow that the crew will follow (e.g., sequential, hierarchical).\n",
        "    )\n",
        "\n",
        "    inputs = {\n",
        "    \"var_1\": var_1,\n",
        "    \"var_2\": var_2,\n",
        "    \"var_3\": var_3\n",
        "    }\n",
        "\n",
        "    result = crew.kickoff(inputs=inputs)\n",
        "    print(\"\\n\\n########################\")\n",
        "    print(\"## Here is your custom crew run result:\")\n",
        "    print(\"########################\\n\")\n",
        "    print(result)\n",
        "\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  result = main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "889206a0"
      },
      "outputs": [],
      "source": [
        "# @title üñ•Ô∏è Display the results of your crew as markdown\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "markdown_text = result.raw  # Adjust this based on the actual attribute\n",
        "\n",
        "# Display the markdown content\n",
        "display(Markdown(markdown_text))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}